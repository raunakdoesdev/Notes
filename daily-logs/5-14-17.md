# 5/14/17 Daily Log

## Double Net Metadata Inference

Finally fixed problem with metadata inference training, and it apperas that the network is actually optimizing and improving itself. Below is the training loss before and after the fix at equal time intervals. Before the training loss wasn't improving at all, after it's been going down steadily. More results will come with the epoch results tommorow. The goal is to surpass the loss we get from the pretrained z2color net on the human set metadata, but this might not be possible until we make the changes described [here](https://github.com/sauhaardac/Notes/blob/master/ideas/dual-network-metadata-inference-idea.md)

| Training Loss Before Fix | Training Loss After Fix |
|--------------------------|-------------------------|
| 0.00482188233873         | 0.00447628520487        |
| 0.00456149974198         | 0.00441443650052        |
| 0.00477338966448         | 0.00428433818161        |
| 0.00484592133434         | 0.0042099625949         |
| 0.00462484947348         | 0.00417008329241        |
| 0.00495147268812         | 0.00418477188214        |
| 0.00472134965821         | 0.0043628910702         |
| 0.00482712858939         | 0.00422861588595        |
| 0.0049041838327          | 0.00427567558945        |
| 0.0047017652163          | 0.00427806650987        |
| 0.00473898829368         | 0.00422981263691        |
| 0.00454661954718         | 0.00408371112426        |
| 0.00477917357057         | 0.00416410532198        |
| 0.00453618342872         | 0.00414339956187        |
| 0.00466203934629         | 0.00418420583301        |
| 0.00447347130685         | 0.00395167554962        |

## Replacing Z2Color with SqueezeNet as the drive network

Most recent losses for SqueezeNet Network are:

```
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May  7 10:16 epoch_save_0.0.00485315997295
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May  8 06:04 epoch_save_1.0.00458513229449
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May  9 03:31 epoch_save_2.0.00445009451694
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May 10 00:42 epoch_save_3.0.00427434084036
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May 10 22:05 epoch_save_4.0.00413679074525
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May 11 19:29 epoch_save_5.0.00404983706991
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May 12 11:00 epoch_save_6.0.00399571756212
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May 13 02:48 epoch_save_7.0.00391657557776
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May 13 18:34 epoch_save_8.0.00387504257538
-rw-rw-r-- 1 tpankaj tpankaj 5.7M May 14 10:29 epoch_save_9.0.00388656647638
```

These are fairly good, it surpasses our best loss so far with Z2Color as a drive network, but we can't truly evaluate it's performance until we drive it.

I tried to bring the car out for a drive today but unfortunately the 3S battery wasn't charging and appears to be dead. Checked with a voltmeter and battery was at 0V and was not charging. This was the first use of the battery since Tushar gave them back to me so I don't know what could have caused it. This was the only 3S Battery we had so I will need to purchase two more (keep one as a backup so work time is not wasted).

- [x] Order two 3S LIPO Batteries on Amazon and submit a reimbursement request.

## TX2 Code Optiization

The code on the TX2 is really messed up right now since I inserted a lot of code to measure the time each part of the network takes to run. I should have created a new branch and made the changes there but unfortuantely I forgot to do this. To clean it all up I can:

1. Move the current timing code to a new folder in the catkin_ws
2. Clone the working code on the master branch of pytorch_inference into the catkin_ws with the same naming as earlier.
3. Create a new branch in the fresh clone
4. Move all of the changes from the folde into the new branch

This may not be the most optimum way to do this, but I'm not amazing at git so I will need to do it this way.

- [ ] Clean up code on the TX2 using above steps

# Human Resources
- [ ] Need to finish Sexual Harrasment thing by the end of the week
- [ ] Need to do the Cyber Security training by the start of June
